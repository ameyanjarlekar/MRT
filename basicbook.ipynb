{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basicbook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameyanjarlekar/MRT/blob/master/basicbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PYfDa3DBT_t9",
        "colab_type": "code",
        "outputId": "bb18f950-82b9-4ddc-b071-f7936c8f2726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ameyanjarlekar/MRT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MRT'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/50)   \u001b[K\rremote: Counting objects:   4% (2/50)   \u001b[K\rremote: Counting objects:   6% (3/50)   \u001b[K\rremote: Counting objects:   8% (4/50)   \u001b[K\rremote: Counting objects:  10% (5/50)   \u001b[K\rremote: Counting objects:  12% (6/50)   \u001b[K\rremote: Counting objects:  14% (7/50)   \u001b[K\rremote: Counting objects:  16% (8/50)   \u001b[K\rremote: Counting objects:  18% (9/50)   \u001b[K\rremote: Counting objects:  20% (10/50)   \u001b[K\rremote: Counting objects:  22% (11/50)   \u001b[K\rremote: Counting objects:  24% (12/50)   \u001b[K\rremote: Counting objects:  26% (13/50)   \u001b[K\rremote: Counting objects:  28% (14/50)   \u001b[K\rremote: Counting objects:  30% (15/50)   \u001b[K\rremote: Counting objects:  32% (16/50)   \u001b[K\rremote: Counting objects:  34% (17/50)   \u001b[K\rremote: Counting objects:  36% (18/50)   \u001b[K\rremote: Counting objects:  38% (19/50)   \u001b[K\rremote: Counting objects:  40% (20/50)   \u001b[K\rremote: Counting objects:  42% (21/50)   \u001b[K\rremote: Counting objects:  44% (22/50)   \u001b[K\rremote: Counting objects:  46% (23/50)   \u001b[K\rremote: Counting objects:  48% (24/50)   \u001b[K\rremote: Counting objects:  50% (25/50)   \u001b[K\rremote: Counting objects:  52% (26/50)   \u001b[K\rremote: Counting objects:  54% (27/50)   \u001b[K\rremote: Counting objects:  56% (28/50)   \u001b[K\rremote: Counting objects:  58% (29/50)   \u001b[K\rremote: Counting objects:  60% (30/50)   \u001b[K\rremote: Counting objects:  62% (31/50)   \u001b[K\rremote: Counting objects:  64% (32/50)   \u001b[K\rremote: Counting objects:  66% (33/50)   \u001b[K\rremote: Counting objects:  68% (34/50)   \u001b[K\rremote: Counting objects:  70% (35/50)   \u001b[K\rremote: Counting objects:  72% (36/50)   \u001b[K\rremote: Counting objects:  74% (37/50)   \u001b[K\rremote: Counting objects:  76% (38/50)   \u001b[K\rremote: Counting objects:  78% (39/50)   \u001b[K\rremote: Counting objects:  80% (40/50)   \u001b[K\rremote: Counting objects:  82% (41/50)   \u001b[K\rremote: Counting objects:  84% (42/50)   \u001b[K\rremote: Counting objects:  86% (43/50)   \u001b[K\rremote: Counting objects:  88% (44/50)   \u001b[K\rremote: Counting objects:  90% (45/50)   \u001b[K\rremote: Counting objects:  92% (46/50)   \u001b[K\rremote: Counting objects:  94% (47/50)   \u001b[K\rremote: Counting objects:  96% (48/50)   \u001b[K\rremote: Counting objects:  98% (49/50)   \u001b[K\rremote: Counting objects: 100% (50/50)   \u001b[K\rremote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/43)   \u001b[K\rremote: Compressing objects:   4% (2/43)   \u001b[K\rremote: Compressing objects:   6% (3/43)   \u001b[K\rremote: Compressing objects:   9% (4/43)   \u001b[K\rremote: Compressing objects:  11% (5/43)   \u001b[K\rremote: Compressing objects:  13% (6/43)   \u001b[K\rremote: Compressing objects:  16% (7/43)   \u001b[K\rremote: Compressing objects:  18% (8/43)   \u001b[K\rremote: Compressing objects:  20% (9/43)   \u001b[K\rremote: Compressing objects:  23% (10/43)   \u001b[K\rremote: Compressing objects:  25% (11/43)   \u001b[K\rremote: Compressing objects:  27% (12/43)   \u001b[K\rremote: Compressing objects:  30% (13/43)   \u001b[K\rremote: Compressing objects:  32% (14/43)   \u001b[K\rremote: Compressing objects:  34% (15/43)   \u001b[K\rremote: Compressing objects:  37% (16/43)   \u001b[K\rremote: Compressing objects:  39% (17/43)   \u001b[K\rremote: Compressing objects:  41% (18/43)   \u001b[K\rremote: Compressing objects:  44% (19/43)   \u001b[K\rremote: Compressing objects:  46% (20/43)   \u001b[K\rremote: Compressing objects:  48% (21/43)   \u001b[K\rremote: Compressing objects:  51% (22/43)   \u001b[K\rremote: Compressing objects:  53% (23/43)   \u001b[K\rremote: Compressing objects:  55% (24/43)   \u001b[K\rremote: Compressing objects:  58% (25/43)   \u001b[K\rremote: Compressing objects:  60% (26/43)   \u001b[K\rremote: Compressing objects:  62% (27/43)   \u001b[K\rremote: Compressing objects:  65% (28/43)   \u001b[K\rremote: Compressing objects:  67% (29/43)   \u001b[K\rremote: Compressing objects:  69% (30/43)   \u001b[K\rremote: Compressing objects:  72% (31/43)   \u001b[K\rremote: Compressing objects:  74% (32/43)   \u001b[K\rremote: Compressing objects:  76% (33/43)   \u001b[K\rremote: Compressing objects:  79% (34/43)   \u001b[K\rremote: Compressing objects:  81% (35/43)   \u001b[K\rremote: Compressing objects:  83% (36/43)   \u001b[K\rremote: Compressing objects:  86% (37/43)   \u001b[K\rremote: Compressing objects:  88% (38/43)   \u001b[K\rremote: Compressing objects:  90% (39/43)   \u001b[K\rremote: Compressing objects:  93% (40/43)   \u001b[K\rremote: Compressing objects:  95% (41/43)   \u001b[K\rremote: Compressing objects:  97% (42/43)   \u001b[K\rremote: Compressing objects: 100% (43/43)   \u001b[K\rremote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 50 (delta 20), reused 13 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sCcHae0fVlxY",
        "colab_type": "code",
        "outputId": "dde25b3f-51e3-4d68-a18f-314fbd0a6241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T3eK9rqmV96Y",
        "colab_type": "code",
        "outputId": "8efe2a0e-c2ed-497d-8dfb-17a4e7a4f94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "new=np.load(\"gdrive/My Drive/outputfinalgreymod.npy\")\n",
        "newa = np.load(\"gdrive/My Drive/outputfinalgreymodans.npy\")\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(new)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(newa)\n",
        "\n",
        "#newa = np.array( [[1,0]]*27600 + [[0,1]]*88784)\n",
        "#sprint(len(a))\n",
        "\n",
        "\n",
        "#outfile.seek(0)\n",
        "#new = np.load(outfile)\n",
        "#print(new.shape)\n",
        "#outfileans.seek(0)\n",
        "#newa = np.load(outfileans)\n",
        "#print(newa.shape)\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout,Reshape\n",
        "from keras import optimizers\n",
        "model = Sequential()\n",
        "model.add(Reshape((50,50,1), input_shape=(50,50)))\n",
        "model.add(Conv2D(64,5,padding='same',input_shape=(50,50,1)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2))\n",
        "model.add(keras.layers.Activation('softmax'))\n",
        "sgd = optimizers.SGD(lr = 0.05, decay = 0.5)\n",
        "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(new,newa,batch_size=2048,epochs=10,validation_split=0.05)\n",
        "# keras library import  for Saving and loading model and weights\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"model_num1_grey.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save(\"model_num3_grey.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100894 samples, validate on 5311 samples\n",
            "Epoch 1/10\n",
            "100894/100894 [==============================] - 41s 411us/step - loss: 0.2966 - acc: 0.9188 - val_loss: 0.1842 - val_acc: 0.9420\n",
            "Epoch 2/10\n",
            "100894/100894 [==============================] - 39s 389us/step - loss: 0.1949 - acc: 0.9363 - val_loss: 0.1782 - val_acc: 0.9454\n",
            "Epoch 3/10\n",
            "100894/100894 [==============================] - 39s 389us/step - loss: 0.1892 - acc: 0.9383 - val_loss: 0.1753 - val_acc: 0.9461\n",
            "Epoch 4/10\n",
            "100894/100894 [==============================] - 39s 389us/step - loss: 0.1857 - acc: 0.9397 - val_loss: 0.1733 - val_acc: 0.9467\n",
            "Epoch 5/10\n",
            "100894/100894 [==============================] - 39s 390us/step - loss: 0.1857 - acc: 0.9399 - val_loss: 0.1719 - val_acc: 0.9465\n",
            "Epoch 6/10\n",
            "100894/100894 [==============================] - 39s 389us/step - loss: 0.1836 - acc: 0.9409 - val_loss: 0.1709 - val_acc: 0.9463\n",
            "Epoch 7/10\n",
            "100894/100894 [==============================] - 39s 388us/step - loss: 0.1817 - acc: 0.9412 - val_loss: 0.1700 - val_acc: 0.9473\n",
            "Epoch 8/10\n",
            "100894/100894 [==============================] - 39s 390us/step - loss: 0.1808 - acc: 0.9413 - val_loss: 0.1691 - val_acc: 0.9477\n",
            "Epoch 9/10\n",
            "100894/100894 [==============================] - 39s 388us/step - loss: 0.1809 - acc: 0.9412 - val_loss: 0.1685 - val_acc: 0.9478\n",
            "Epoch 10/10\n",
            "100894/100894 [==============================] - 39s 389us/step - loss: 0.1802 - acc: 0.9419 - val_loss: 0.1680 - val_acc: 0.9484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZzHUL-KvBfSH",
        "colab_type": "code",
        "outputId": "d360bbaa-daef-4f1a-c937-763157e30e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1094
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout,Reshape\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "\n",
        "inputa = np.load(\"gdrive/My Drive/betterdatagrey.npy\")\n",
        "output = np.load(\"gdrive/My Drive/betterdatagreyout.npy\")\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(inputa)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(output)\n",
        "\n",
        "new_model = load_model('gdrive/My Drive/model_num3_grey.h5')\n",
        "for layer in new_model.layers:\n",
        "   layer.trainable = False\n",
        "model.fit(inputa,output,batch_size=128,epochs=50,validation_split=0.1)\n",
        "\n",
        "\n",
        "#from keras.models import model_from_json\n",
        "#from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "#model_json = model.to_json()\n",
        "\n",
        "\n",
        "#with open(\"newmodel_num1_greyoutput.json\", \"w\") as json_file:\n",
        "#    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "#model.save(\"new_model_num3_greyoutput.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4044 samples, validate on 450 samples\n",
            "Epoch 1/50\n",
            "4044/4044 [==============================] - 2s 525us/step - loss: 0.2744 - acc: 0.8976 - val_loss: 0.2328 - val_acc: 0.9156\n",
            "Epoch 2/50\n",
            "4044/4044 [==============================] - 2s 503us/step - loss: 0.2731 - acc: 0.9028 - val_loss: 0.2326 - val_acc: 0.9156\n",
            "Epoch 3/50\n",
            "4044/4044 [==============================] - 2s 507us/step - loss: 0.2772 - acc: 0.8969 - val_loss: 0.2323 - val_acc: 0.9156\n",
            "Epoch 4/50\n",
            "4044/4044 [==============================] - 2s 507us/step - loss: 0.2768 - acc: 0.8954 - val_loss: 0.2319 - val_acc: 0.9156\n",
            "Epoch 5/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1185c08d4cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m    \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QQUVe2ciU93C",
        "colab_type": "code",
        "outputId": "feb43b60-bac9-4f55-a63d-c7f339f8ecaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout,Reshape\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "\n",
        "inputa = np.load(\"gdrive/My Drive/betterdata.npy\")\n",
        "output = np.load(\"gdrive/My Drive/betterdataout.npy\")\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(inputa)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(output)\n",
        "\n",
        "new_model = load_model('gdrive/My Drive/model_num_1_rgb.h5')\n",
        "for layer in new_model.layers:\n",
        "   layer.trainable = False\n",
        "model.fit(inputa,output,batch_size=128,epochs=50,validation_split=0.1)\n",
        "\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"coloured_model_num1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save(\"new_model_num_coluoured.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4494, 50, 50, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d08364b38dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gdrive/My Drive/betterdataout.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrng_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inputb' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JbgLF7LfAUH8",
        "colab_type": "code",
        "outputId": "fc28c0ba-12f6-443d-d208-049727317723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "new2=np.load(\"gdrive/My Drive/outputfinal.npy\")\n",
        "new2a = np.array( [[1,0]]*27600 + [[0,1]]*88784)\n",
        "#newa = np.load(\"MRT/outputfinalans1.npy\")\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(new2)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(new2a)\n",
        "#sprint(len(a))\n",
        "\n",
        "\n",
        "#outfile.seek(0)\n",
        "#new = np.load(outfile)\n",
        "#print(new.shape)\n",
        "#outfileans.seek(0)\n",
        "#newa = np.load(outfileans)\n",
        "#print(newa.shape)\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout,Reshape\n",
        "from keras import optimizers\n",
        "model = Sequential()\n",
        "#model.add(Reshape((50,50,1), input_shape=(50,50)))\n",
        "model.add(Conv2D(64,5,padding='same',input_shape=(50,50,3)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64,5,padding='same'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2))\n",
        "model.add(keras.layers.Activation('softmax'))\n",
        "sgd = optimizers.SGD(lr = 0.05, decay = 0.5)\n",
        "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(new2,new2a,batch_size=2048,epochs=10,validation_split=0.05)\n",
        "# keras library import  for Saving and loading model and weights\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"model_num_rgb.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save(\"model_num_1_rgb.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110564 samples, validate on 5820 samples\n",
            "Epoch 1/10\n",
            "110564/110564 [==============================] - 54s 491us/step - loss: 0.3632 - acc: 0.8939 - val_loss: 0.2164 - val_acc: 0.9184\n",
            "Epoch 2/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.2139 - acc: 0.9242 - val_loss: 0.2016 - val_acc: 0.9290\n",
            "Epoch 3/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.2038 - acc: 0.9284 - val_loss: 0.1923 - val_acc: 0.9330\n",
            "Epoch 4/10\n",
            "110564/110564 [==============================] - 48s 432us/step - loss: 0.1982 - acc: 0.9312 - val_loss: 0.1868 - val_acc: 0.9349\n",
            "Epoch 5/10\n",
            "110564/110564 [==============================] - 47s 430us/step - loss: 0.1947 - acc: 0.9327 - val_loss: 0.1820 - val_acc: 0.9375\n",
            "Epoch 6/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.1924 - acc: 0.9340 - val_loss: 0.1788 - val_acc: 0.9383\n",
            "Epoch 7/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.1893 - acc: 0.9352 - val_loss: 0.1755 - val_acc: 0.9400\n",
            "Epoch 8/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.1873 - acc: 0.9361 - val_loss: 0.1732 - val_acc: 0.9411\n",
            "Epoch 9/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.1868 - acc: 0.9362 - val_loss: 0.1719 - val_acc: 0.9418\n",
            "Epoch 10/10\n",
            "110564/110564 [==============================] - 48s 430us/step - loss: 0.1847 - acc: 0.9373 - val_loss: 0.1697 - val_acc: 0.9433\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}